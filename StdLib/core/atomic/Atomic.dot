
module std.atomic

public enum MemoryOrder {
    case NotAtomic = 0
    case Relaxed = 1

    /// load-consume
    case Consume = 2

    /// load-acquire
    case Acquire = 3

    /// store-release
    case Release = 4

    /// store-release load-acquire
    case AcquireRelease = 5

    /// store-release load-acquire
    case SequentiallyConsistent = 6
}

/// Derived StringRepresentable conformance.
extend MemoryOrder with StringRepresentable {}

@compiletime @testable
public struct Atomic<T: ?> {
    /// The underlying value.
    private var value: T

    /// C'tor. Initialize from a T.
    init (_ value: T) {
        self.value = value
    }
    
    /// Check that the given memory order is valid for a load.
    private static def checkValidLoadOrder(_ memoryOrder: MemoryOrder) {
        match memoryOrder {
        default:
            return
        case .Release:
        case .AcquireRelease:
        case .NotAtomic:
            sys.reportFatalError("memory order $memoryOrder is invalid for a load")
        }
    }

    /// Check that the given memory order is valid for a store.
    private static def checkValidStoreOrder(_ memoryOrder: MemoryOrder) {
        match memoryOrder {
        default:
            return
        case .Consume:
        case .Acquire:
        case .AcquireRelease:
        case .NotAtomic:
            sys.reportFatalError("memory order $memoryOrder is invalid for a store")
        }
    }

    /// Check that the given memory order is valid for an exchange.
    private static def checkValidExchangeOrder(_ memoryOrder: MemoryOrder) {
         match memoryOrder {
        default:
            return
        case .Release:
        case .AcquireRelease:
        case .NotAtomic:
            sys.reportFatalError("memory order $memoryOrder is invalid for an exchange")
        }
    }

    /// Load the value with the given memory order.
    def load(memoryOrder: MemoryOrder = .SequentiallyConsistent) -> T {
        checkValidLoadOrder(memoryOrder)
        return builtin.atomic_load(builtin.addressOf(self.value),
                                   memoryOrder as i64 as i32)
    }

    /// Store @param{storeVal} to the underyling memory address.
    def mutating store(_ storeVal: owned T,
                       memoryOrder: MemoryOrder = .SequentiallyConsistent) {
        checkValidStoreOrder(memoryOrder)
        return builtin.atomic_store(storeVal, builtin.addressOf(&self.value),
                                    memoryOrder as i64 as i32)
    }

    /// Expands to an instruction that atomically checks whether a specified value 
    /// is in a memory location, and, if it is, stores a new value.
    /// \return the value that was loaded.
    @discardableResult
    def mutating cmpxchg(compareTo cmp: T, exchangeWith newVal: T,
                         successOrdering: MemoryOrder = .SequentiallyConsistent,
                         failureOrdering: MemoryOrder = .SequentiallyConsistent) -> T {
        checkValidExchangeOrder(successOrdering)
        checkValidExchangeOrder(failureOrdering)

        return builtin.atomic_cmpxchg(builtin.addressOf(&self.value), cmp, newVal,
                                      successOrdering as i64 as i32,
                                      failureOrdering as i64 as i32)
    }
}

extend Atomic with Comparable /* where T is Comparable */ {
    /// Three-way comparison operator.
    /// \return -1 iff @code{self < rhs}, 0 if @code{self == rhs}, 1 otherwise
    def infix <=>(rhs: Self) -> Int {
        return (self.load() <=> rhs.load()) as Int
    }

    def infix <=>(rhs: T) -> Int {
        return (self.load() <=> rhs) as Int
    }

    macro define_proxy_cmp_fn{
        ($($op:tok)*) => { $(
            def infix $op(rhs: T) -> Bool {
                return self.load() $op rhs
            }
        )... }
    }

    define_proxy_cmp_fn!{ == != < > <= >= }
}

alias IsNumeric<T> = IsIntegral<T> || IsFloating<T>

extend Atomic where IsNumeric<T> {
    macro define_rmw_op{
        ($(($name:tok, $enumOp:tok, $op:tok))*) => {
            $(
                def $name(_ rhs: T, order: MemoryOrder = .SequentiallyConsistent) -> T {
                    var cpy = self.copy()
                    _ = builtin.atomic_rmw(.$enumOp, builtin.addressOf(cpy.value), rhs,
                                           order as i64 as i32)

                    return cpy
                }

                def infix $op(rhs: T) -> T {
                    return $name(rhs)
                }

                def infix ${$op=}(rhs: T) {
                    _ = builtin.atomic_rmw(.$enumOp, builtin.addressOf(self.value), rhs,          
                                           MemoryOrder.SequentiallyConsistent as i64 as i32)
                }
            )...
        }
    }

    define_rmw_op!{ (add, Add, +) (sub, Sub, -) (and, And, &) (or, Or, |) (xor, Xor, ^) }

    /// Increment the contained value by one.
    /// \return the incremented value.
    @discardableResult def prefix ++() -> T {
        self += 1 as T
        return load()
    }

    /// Increment the contained value by one.
    /// \return the value before incrementing.
    def postfix ++() -> T {
        var cpy = load()
        self += 1 as T

        return cpy
    }

    /// Decrement the contained value by one.
    /// \return the decremented value.
    @discardableResult def prefix --() -> T {
        self -= 1 as T
        return load()
    }

    /// Decrement the contained value by one.
    /// \return the value before decrementing.
    def postfix --() -> T {
        var cpy = load()
        self -= 1 as T
        
        return cpy
    }
}

// extend Atomic where IsIntegral<T> {
//     define_rmw_op!{ (add, Add, +) (sub, Sub, -) (and, And, &) (or, Or, |) (xor, Xor, ^) }
// }

// extend Atomic where IsFloating<T> {
//     define_rmw_op!{ (add, Add, +) (sub, Sub, -) }
// }

extend Atomic with Copyable, ImplicitlyCopyable {}
